<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Dennis Nguyen]]></title>
  <link href="http://dnguy078.github.io/atom.xml" rel="self"/>
  <link href="http://dnguy078.github.io/"/>
  <updated>2014-07-28T16:38:16-07:00</updated>
  <id>http://dnguy078.github.io/</id>
  <author>
    <name><![CDATA[Dennis Nguyen]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Amazon EMR Step Process]]></title>
    <link href="http://dnguy078.github.io/blog/2014/07/21/amazon-emr-step-process/"/>
    <updated>2014-07-21T11:12:24-07:00</updated>
    <id>http://dnguy078.github.io/blog/2014/07/21/amazon-emr-step-process</id>
    <content type="html"><![CDATA[<p>I wrote a hive script to backup an Amazon dynamodb table to an Amazon S3 table a
few weeks ago. The dynamodb table contained over 8 billion log records that span
several years.</p>

<p>Today, I wrote a java application to initialize an EC2 instance and boostrapped
it with Hadoop Hive. The java application allows me to run the hive script. In
addition, I wrote a cronjob to allow the java application to be executed daily
to allow for daily backups for the dynamodb table.</p>

<figure class='code'><figcaption><span> (steps)</span> <a href='http://dnguy078.github.io/code/steps'>download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
</pre></td><td class='code'><pre><code class=''><span class='line'><span class="n">private</span> <span class="k">static</span> <span class="n">RunJobFlowResult</span> <span class="nf">executeHiveScript</span><span class="p">(</span><span class="n">String</span> <span class="n">dir</span><span class="p">,</span> <span class="n">String</span> <span class="n">startDate</span><span class="p">,</span> <span class="n">String</span> <span class="n">endDate</span><span class="p">,</span> <span class="n">String</span> <span class="n">monthDate</span><span class="p">,</span> <span class="n">String</span> <span class="n">weekStart</span><span class="p">,</span> <span class="n">String</span> <span class="n">weekEnd</span><span class="p">)</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="n">StepFactory</span> <span class="n">stepFactory</span> <span class="o">=</span> <span class="n">new</span> <span class="n">StepFactory</span><span class="p">();</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">StepConfig</span> <span class="n">enabledebugging</span> <span class="o">=</span> <span class="n">new</span> <span class="n">StepConfig</span><span class="p">()</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withName</span><span class="p">(</span><span class="s">&quot;Enable debugging&quot;</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withActionOnFailure</span><span class="p">(</span><span class="s">&quot;TERMINATE_JOB_FLOW&quot;</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withHadoopJarStep</span><span class="p">(</span><span class="n">stepFactory</span><span class="p">.</span><span class="n">newEnableDebuggingStep</span><span class="p">());</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">StepConfig</span> <span class="n">installHive</span> <span class="o">=</span> <span class="n">new</span> <span class="n">StepConfig</span><span class="p">()</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withName</span><span class="p">(</span><span class="s">&quot;Install Hive&quot;</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withActionOnFailure</span><span class="p">(</span><span class="s">&quot;TERMINATE_JOB_FLOW&quot;</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withHadoopJarStep</span><span class="p">(</span><span class="n">stepFactory</span><span class="p">.</span><span class="n">newInstallHiveStep</span><span class="p">());</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>  <span class="n">HadoopJarStepConfig</span> <span class="n">customJarStep</span> <span class="o">=</span> <span class="n">new</span> <span class="n">HadoopJarStepConfig</span><span class="p">(</span><span class="s">&quot;s3:testJarPath&quot;</span><span class="p">);</span>
</span><span class='line'>  <span class="n">customJarStep</span><span class="p">.</span><span class="n">withArgs</span><span class="p">(</span><span class="s">&quot;1000&quot;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'> <span class="n">StepConfig</span> <span class="n">setReadCap</span> <span class="o">=</span> <span class="n">new</span> <span class="n">StepConfig</span><span class="p">()</span>
</span><span class='line'>          <span class="p">.</span><span class="n">withName</span><span class="p">(</span><span class="s">&quot;Setting up Read Capacity&quot;</span><span class="p">)</span>
</span><span class='line'>          <span class="p">.</span><span class="n">withActionOnFailure</span><span class="p">(</span><span class="s">&quot;TERMINATE_JOB_FLOW&quot;</span><span class="p">)</span>
</span><span class='line'>          <span class="p">.</span><span class="n">withHadoopJarStep</span><span class="p">(</span><span class="n">customJarStep</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'> <span class="n">StepConfig</span> <span class="n">hiveScript</span> <span class="o">=</span> <span class="n">new</span> <span class="n">StepConfig</span><span class="p">().</span><span class="n">withName</span><span class="p">(</span><span class="s">&quot;Hive Script&quot;</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withActionOnFailure</span><span class="p">(</span><span class="s">&quot;TERMINATE_JOB_FLOW&quot;</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withHadoopJarStep</span><span class="p">(</span><span class="n">stepFactory</span><span class="p">.</span><span class="n">newRunHiveScriptStep</span><span class="p">(</span><span class="s">&quot;s3://testPathToHiveScript/testScript&quot;</span><span class="p">,</span>
</span><span class='line'>                 <span class="s">&quot;-d&quot;</span><span class="p">,</span><span class="s">&quot;s3_output_bucket=s3://testPathtoS3Dir&quot;</span> <span class="o">+</span> <span class="n">dir</span> <span class="p">,</span>
</span><span class='line'>                 <span class="s">&quot;-d&quot;</span><span class="p">,</span> <span class="s">&quot;start_date={</span><span class="se">\&quot;</span><span class="s">s</span><span class="se">\&quot;</span><span class="s">:</span><span class="se">\&quot;</span><span class="s">&quot;</span>  <span class="o">+</span> <span class="n">startDate</span> <span class="o">+</span> <span class="s">&quot;</span><span class="se">\&quot;</span><span class="s">}&quot;</span><span class="p">,</span>
</span><span class='line'>                 <span class="s">&quot;-d&quot;</span><span class="p">,</span> <span class="s">&quot;end_date={</span><span class="se">\&quot;</span><span class="s">s</span><span class="se">\&quot;</span><span class="s">:</span><span class="se">\&quot;</span><span class="s">&quot;</span> <span class="o">+</span> <span class="n">endDate</span> <span class="o">+</span> <span class="s">&quot;</span><span class="se">\&quot;</span><span class="s">}&quot;</span><span class="p">,</span>
</span><span class='line'>                 <span class="s">&quot;-d&quot;</span><span class="p">,</span> <span class="s">&quot;month_date={</span><span class="se">\&quot;</span><span class="s">s</span><span class="se">\&quot;</span><span class="s">:</span><span class="se">\&quot;</span><span class="s">&quot;</span><span class="o">+</span> <span class="n">monthDate</span> <span class="o">+</span><span class="s">&quot;</span><span class="se">\&quot;</span><span class="s">}&quot;</span><span class="p">,</span>
</span><span class='line'>                 <span class="s">&quot;-d&quot;</span><span class="p">,</span><span class="s">&quot;week_start={</span><span class="se">\&quot;</span><span class="s">s</span><span class="se">\&quot;</span><span class="s">:</span><span class="se">\&quot;</span><span class="s">&quot;</span> <span class="o">+</span> <span class="n">weekStart</span> <span class="o">+</span> <span class="s">&quot;</span><span class="se">\&quot;</span><span class="s">}&quot;</span><span class="p">,</span>
</span><span class='line'>                 <span class="s">&quot;-d&quot;</span><span class="p">,</span><span class="s">&quot;week_end={</span><span class="se">\&quot;</span><span class="s">s</span><span class="se">\&quot;</span><span class="s">:</span><span class="se">\&quot;</span><span class="s">&quot;</span> <span class="o">+</span> <span class="n">weekEnd</span>  <span class="o">+</span><span class="s">&quot;</span><span class="se">\&quot;</span><span class="s">}&quot;</span><span class="p">)</span>
</span><span class='line'>                 <span class="p">);</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'> <span class="n">RunJobFlowRequest</span> <span class="n">request</span> <span class="o">=</span> <span class="n">new</span> <span class="n">RunJobFlowRequest</span><span class="p">()</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withName</span><span class="p">(</span><span class="s">&quot;Month backup test  hadoop version for (m1.large) Storage throughput 1.0 : &quot;</span> <span class="o">+</span> <span class="n">monthDate</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withSteps</span><span class="p">(</span><span class="n">enabledebugging</span><span class="p">,</span> <span class="n">installHive</span><span class="p">,</span> <span class="n">setReadCap</span> <span class="p">,</span> <span class="n">hiveScript</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withLogUri</span><span class="p">(</span><span class="s">&quot;s3://pathtoLogfiles&quot;</span> <span class="o">+</span> <span class="n">dir</span><span class="p">)</span>
</span><span class='line'>         <span class="p">.</span><span class="n">withInstances</span><span class="p">(</span><span class="n">new</span> <span class="n">JobFlowInstancesConfig</span><span class="p">()</span>
</span><span class='line'>              <span class="p">.</span><span class="n">withEc2KeyName</span><span class="p">(</span><span class="s">&quot;fakeec2key&quot;</span><span class="p">)</span>
</span><span class='line'>              <span class="c1">//1.0.3 works</span>
</span><span class='line'>              <span class="p">.</span><span class="n">withHadoopVersion</span><span class="p">(</span><span class="s">&quot;2.4.0&quot;</span><span class="p">)</span>
</span><span class='line'>              <span class="p">.</span><span class="n">withInstanceCount</span><span class="p">(</span><span class="mi">41</span><span class="p">)</span>
</span><span class='line'>              <span class="p">.</span><span class="n">withKeepJobFlowAliveWhenNoSteps</span><span class="p">(</span><span class="nb">false</span><span class="p">)</span>
</span><span class='line'>              <span class="p">.</span><span class="n">withMasterInstanceType</span><span class="p">(</span><span class="s">&quot;m1.large&quot;</span><span class="p">)</span>
</span><span class='line'>              <span class="p">.</span><span class="n">withSlaveInstanceType</span><span class="p">(</span><span class="s">&quot;m1.large&quot;</span><span class="p">)</span>
</span><span class='line'>               <span class="p">);</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'> <span class="n">RunJobFlowResult</span> <span class="n">result</span> <span class="o">=</span> <span class="n">emr</span><span class="p">.</span><span class="n">runJobFlow</span><span class="p">(</span><span class="n">request</span><span class="p">);</span>
</span><span class='line'> <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sendgrid New Office]]></title>
    <link href="http://dnguy078.github.io/blog/2014/07/17/sendgrid-new-office/"/>
    <updated>2014-07-17T22:56:22-07:00</updated>
    <id>http://dnguy078.github.io/blog/2014/07/17/sendgrid-new-office</id>
    <content type="html"><![CDATA[<p>hello</p>

<p><video width='540' height='320' preload='none' controls poster='https://dl.dropboxusercontent.com/u/73868732/VID_28160707_112812.mkv'><source src='http://dl.dropboxusercontent.com/u/73868732/VID_28160707_112812.mkv' ></video>
<img src="http://dl.dropboxusercontent.com/u/73868732/image.jpeg"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Hive Backup]]></title>
    <link href="http://dnguy078.github.io/blog/2014/06/30/hadoop-hive/"/>
    <updated>2014-06-30T22:34:17-07:00</updated>
    <id>http://dnguy078.github.io/blog/2014/06/30/hadoop-hive</id>
    <content type="html"><![CDATA[<p>I wrote a Hive script to backup a schema-less dynamodb database to S3.</p>

<figure class='code'><figcaption><span> (backupDailyScript)</span> <a href='http://dnguy078.github.io/code/backupDailyScript'>download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>--####################################################################
</span><span class='line'>--# Export schema-less table from DDB to S3
</span><span class='line'>--#
</span><span class='line'>--# Params:
</span><span class='line'>--#    DYNAMODB_INPUT_TABLE - name of input table
</span><span class='line'>--#    s3_output_bucket - output bucket s3 path
</span><span class='line'>--#    DYNAMODB_READ_PERCENT - percent of table RCU to use
</span><span class='line'>--#    DYNAMODB_ENDPOINT - dynamodb service endpoint to us
</span><span class='line'>--#    start_date - start date ie) 2014-07-01
</span><span class='line'>--#    end_date - end date ie) 2014-08-01
</span><span class='line'>--#    month_date - month date ie) m2014-07
</span><span class='line'>--#    week_start - week start calculated from start_date ie) w2014-27
</span><span class='line'>--#    week_end - week end calculated from end_date ie) w2014-33
</span><span class='line'>--####################################################################
</span><span class='line'>
</span><span class='line'>SET dynamodb.endpoint=dynamodb.us-east-1.amazonaws.com;
</span><span class='line'>SET dynamodb.throughput.read.percent = .4;
</span><span class='line'>--INCREASES Parallelism across the cluster
</span><span class='line'>SET mapred.max.split.size=1000000;
</span><span class='line'>
</span><span class='line'>-- Drop tables
</span><span class='line'>DROP table dynamodb_table;
</span><span class='line'>DROP table s3_table;
</span><span class='line'>
</span><span class='line'>--create table from dyno
</span><span class='line'>CREATE EXTERNAL TABLE dynamodb_table (item map<span class="nt">&lt;string</span><span class="err">,string</span><span class="nt">&gt;</span>)
</span><span class='line'>STORED BY &#39;org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler&#39;
</span><span class='line'>TBLPROPERTIES (&quot;dynamodb.table.name&quot; = &quot;stats&quot;,
</span><span class='line'>&quot;dynamodb.throughput.read.percent&quot; = &quot;.4&quot;);
</span><span class='line'>
</span><span class='line'>-- Create table in S3
</span><span class='line'>CREATE EXTERNAL TABLE s3_table (item map<span class="nt">&lt;string</span><span class="err">,string</span><span class="nt">&gt;</span>)
</span><span class='line'>ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\001&#39;
</span><span class='line'>MAP KEYS TERMINATED BY &#39;\003&#39;
</span><span class='line'>LINES TERMINATED BY &#39;\n&#39;
</span><span class='line'>--specifies location of backup
</span><span class='line'>LOCATION &#39;<span class="cp">${</span><span class="n">s3_output_bucket</span><span class="cp">}</span>&#39;;
</span><span class='line'>
</span><span class='line'>-- Load S3 Table with data from DynamoDB
</span><span class='line'>INSERT OVERWRITE DIRECTORY &#39;<span class="cp">${</span><span class="n">s3_output_bucket</span><span class="cp">}</span>&#39; SELECT item FROM dynamodb_table WHERE
</span><span class='line'>(item[&quot;date&quot;] = &#39;<span class="cp">${</span><span class="n">start_date</span><span class="cp">}</span>&#39;);
</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Hive]]></title>
    <link href="http://dnguy078.github.io/blog/2014/06/25/hadoop-hive/"/>
    <updated>2014-06-25T14:31:58-07:00</updated>
    <id>http://dnguy078.github.io/blog/2014/06/25/hadoop-hive</id>
    <content type="html"><![CDATA[<p>I spent the last few days learning about Hadoop Hive. I&rsquo;ll give a brief summary
of what I&rsquo;ve learned about Hadoop Hive.</p>

<p>Hive is a data warehouse system for Hadoop.  allows for analysis of large datasets stored in Hadoop&rsquo;s HDFS (Hadoop Distributed File System). It provides
an SQL-like language for querying data called HiveQL. When you execute a querying with Hive, the query gets abstracted into MapReduce jobs that could be executed across a cluster of computers.</p>

<p>Why Hive?
Hive allows for developers to write UDF (User Defined Functions). UDFs allows
you to write simple scripts written in Java or Python that control the Map
Reduce transformation stages. It can difficult to write MapReduce jobs in Java.
Hive makes it easier to query and analyse large datasets. Hive will outperform
classic databases like PostgreSQL for large datasets over 10GB.</p>

<p>Data can be added to the HDFS and you can create an &ldquo;EXTERNAL TABLE&rdquo; from
multiple sources include Amazon S3 or dynamodb. The &ldquo;EXTERNAL TABLE&rdquo; allows you
to load database into HDFS to query.</p>

<p>According to HortonWorks, there are works to improve Hive queries by 100x with
the Stinger project, YARN, and Hadoop 2.0. Certainly these will allow for a
faster Hive query.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Something Awesome!]]></title>
    <link href="http://dnguy078.github.io/blog/2014/06/21/something-awesome/"/>
    <updated>2014-06-21T11:12:24-07:00</updated>
    <id>http://dnguy078.github.io/blog/2014/06/21/something-awesome</id>
    <content type="html"><![CDATA[<p>Honest. Hungry. Humble. Happy. These are the four core values that were introduced to
me on my first day of my summer internship at SendGrid. I hope to live up to
these core values throughout my internship and beyond. I am thrilled to work
here for the summer. I will be working in the R&amp;D software engineering
department. I believe SendGrid will be the place that allows me to grow and
develop into a great software engineer. My two goals for this summer is to
develop clean, usable, and scalable code.  I also want improve my communication
skills when working with others and teams.</p>

<p>I started the day taking care of HR work for new employees. After settling into
workstation, I grabbed lunch with my Sujit, Arielle, Kane, and my manager Kien.
Arielle treated me out to lunch at the Green Tomato Grill.</p>

<p>After lunch, Kien walked me through setting up my environments for working with
the sgstats-hadoop, sgstats-web, and sgstats-rest. While we set up the
evironments, Kien gave a quick introduction to using git ( branch, commit, push,
pull, fork). I was also given a quick run through the existing production code
which is written in python. Python seems like a very intuitive language. The
code is simple and seems very straight forward.</p>

<p>Kien gave a overview of the Hadoop framework and introduced me to MapReduce. Map
is procedure that performs a filtering/sorting. Reduce performs a summary
operation from the Map function. The Hadoop framework is becoming increasingly
popular when working with Big Data. SendGrid uses Hadoop to provide analytics
for their email services and determining billing for their clients. One of the
projects I will be working on for the summer involves using Hadoop to filter
through logs and backing up the billings stats logs. I was also exposed to
Apache&rsquo;s Kafka. I will continue to learn more about Kafka.</p>

<p>Kien and I also worked on backing up a dynoDB to a S3 database. We managed to
backup a small dynoDB to S3 database during our testing. Amazon also uses its
own MapReduce, called EMR, to backup these databases.</p>

<p>Tomorrow, I will be continue to work on the dynoDB and S3 databases. I will also
learn about using Chef&rsquo;s Cookbook. First day was amazing. Everyone is friendly,
hard working, and fun. Heres to a great summer!</p>
]]></content>
  </entry>
  
</feed>
